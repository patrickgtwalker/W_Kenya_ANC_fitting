---
title: "Assessing Malaria Prevalence: Community vs Antenatal Care in Rarieda Sub-county, Western Kenya"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
source("setup.R")
source("functions.R")
source("data_generation.R")
source("models.R")
source("plot_functions.R")
set.seed(100)
```

## **Introduction**

This analysis summarises the key features of our approach to assessing the relationship between the prevalence of malaria within antenatal care (ANC) relative to that in the community in Rarieda sub-county, Western Kenya. We will first show how we fit and compare models of differing complexity. Then, we will demonstrate how to generate out-of-sample predictions for settings based on ANC prevalence alone and quantify the incremental value of the data compared to information typically available via population-based surveys of malaria prevalence in the community.

## **Generating Synthetic Data**

We begin by generating synthetic data representing the simplest and most complex models.

### **Model 1 - Simplest Model**

In this model, prevalence varies by site and month with a constant log-odds difference between ANC and community prevalence.

```{r Generate data from Model 1}
# Generating data for Model 1
m1_data <- generate_data_m1()
# Plotting the data
plot_m1_data<-get_data_plots(m1_data$data_to_model)
plot_m1_data$overall_plot
```

### **Model 5 - Most Complex Model**

In this model, prevalence varies by site and month with a linear relationship on the logit-scale, having different slopes and intercepts depending upon gravidity category.

```{r Generate data from Model 5}
# Generating data for Model 5
m5_data <- generate_data_m5()
# Plotting the data
plot_m5_data<-get_data_plots(m5_data$data_to_model)
plot_m5_data$overall_plot
```

## **Fitting Models**

Next, we fit the models to the generated data.

### **Fit Model 1**

```{r Fit Model 1}
m1_fitting <- fit_m1(m1_data$data_to_model, 100, 100, 10)
```

### **Fit Model 5**

```{r Fit Model 5}
m5_fitting <- fit_m5(m5_data$data_to_model, 100, 100, 10)
```

## **Model Evaluation**

We evaluate the models by comparing the fitted parameters to the simulated values.

### **Model 1 Evaluation**

```{r summarise Model 1 fit}
m1_plots <- get_fit_plots(m1_fitting, m1_data$simulated_survey_data, m1_data$param_df)
m1_plots$overall_plot
```

### **Model 5 Evaluation**

```{r summarise Model 5 fit}
m5_plots <- get_fit_plots(m5_fitting, m5_data$simulated_survey_data, m5_data$param_df)
m5_plots$overall_plot
```

## **Fitting Simple Model to Complex Data**

We fit the simplest model to data generated by the most complex model to highlight fitting metrics.

```{r fit simpler model to data generated with more complex model}
m1_fit_to_m5_data <- fit_m1(m5_data$data_to_model, 1000, 1000, 1)
m1_fit_m5_data_plots <- get_fit_plots(m1_fit_to_m5_data, m5_data$simulated_survey_data, m5_data$param_df)
m1_fit_m5_data_plots$overall_plot
```

### **Model Comparison**

We compare the effective number of parameters and the DIC between models. Here the difference between complexity of the model could be expected to be approximately (though not exactly) six, reflecting the difference in number of specified parameters. DIC is then a metric which balances the improvement of fit versus the potential for over-fitting of the more complex model- here fitting Model 1 to the data generated by Model 5 produces a substantially worse fit than fitting the correct model as one would expect.

```{r compare fitting metrics}
## difference in effective parameters 
m5_fitting$p_d - m1_fit_to_m5_data$p_d
m5_fitting$DIC - m1_fit_to_m5_data$DIC
```

## **Out-of-Sample Predictions**

We now assess the extent to which models can predict out-of-sample. Here, we reconstruct ANC prevalence between surveys spaced over a few months every few years.

When fitting our models, we use random effects for site and month to capture the inherent variability and unobserved heterogeneity across different sites and months. Random effects allow us to generalize our findings beyond the specific sites and months in our dataset, providing a more robust model that accounts for potential variability in new data.

However, for reconstructing community prevalence in months or sites in which data are not available, we want to relax the assumption of normality in these effects. To demonstrate this, we simulate log-odds ratios (log ORs) by month (if predicting month) or by site (if predicting site) that are non-normally distributed, specifically using a uniform distribution from 0 to 1. The model for prediction then uses fixed effects to provide the maximum flexibility in prediction based on the observed data. This approach allows us to capture the full range of possible values without being constrained by the normality assumption.

### **Generate Data for reconstructing data by month**

```{r Generate data for monthly predictions}
## will generate a dataset with a default of only including community prevalence for 3 months every 3 years
months_to_reconstruct <- generate_data_m1_runif_month()
```

### Reconstructing 

```{r reconstruct monthly}
### 
reconstruct_months <- predict_months_m1(months_to_reconstruct$data_to_model, 500, 1000, 5)
reconstruct_months_summary <- get_fit_plots(reconstruct_months, months_to_reconstruct$simulated_survey_data, months_to_reconstruct$param_df, pred_months = months_to_reconstruct$pred_months)
reconstruct_months_summary$overall_plot
```

### Similarly reconstructing site-level community prevalence using only ANC data

```{r reconstruct site}
## default just arbitralily removes survey data in sites 15-20 from fitting
sites_to_reconstruct <- generate_data_m1_runif_site()

reconstruct_sites <- predict_sites_m1(sites_to_reconstruct$data_to_model, 100, 200, 1)
reconstruct_sites_summary <- get_fit_plots(reconstruct_sites, sites_to_reconstruct$simulated_survey_data, sites_to_reconstruct$param_df, pred_sites = sites_to_reconstruct$pred_sites)
reconstruct_sites_summary$overall_plot
```

## **Quantitative Assessment of our reconstructions of community prevalence**

We present two metrics to assess the predictive accuracy: Residual Mean Square Error (RMSE) and Continuous Ranked Probability Score (CRPS).

### **RMSE**

RMSE measures the absolute difference between predicted and observed prevalence. It penalizes predictions that are further away more heavily. This makes RMSE particularly sensitive to outliers and large errors, which can be both a strength and a limitation.

Strengths:

-   RMSE is straightforward to interpret: lower RMSE values indicate better model performance.

-   As a distance measure involving % prevalence it is likely more intuitive and involves a metric typically used to stratify endemicity levels.

-   Heavily penalizes large errors, which can be useful in applications where such errors are particularly costly.

Limitations:

-   RMSE is sensitive to outliers, which can disproportionately affect the metric.

-   RMSE typically isn't recommended for assessing the validity of a probability metric. For example, in a situation where a prediction ranges between 47% and 53% for a 50% prevalence, this may be reasonable. However, for a setting with 1% prevalence, predictions ranging from -2% to 4% are substantially less useful, yet all four of these predictions have the same RMSE.

-   It is not a proper scoring rule for probabilistic forecasts because it does not account for the predicted distribution's uncertainty.

```{r summarise monthly reconstruction }
#RMSE for every site-month
reconstruct_months_summary$prediction_metrics %>%   filter(month != "all", site != "all") %>%   summarise(mean_pc_RMSE = mean(RMSE * 100)) 
#RMSE for overall monthly prevalence 
reconstruct_months_summary$prediction_metrics %>%   filter(site == "all") %>%   summarise(mean_pc_RMSE_over_sites = mean(RMSE * 100)) 
```

### **CRPS**

CRPS is a more technically valid metric for assessing the accuracy of probability predictions. It generalizes the concept of RMSE to probabilistic forecasts, providing a more nuanced view.

Strengths:

-   CRPS is a proper scoring rule, meaning it appropriately accounts for the predicted distribution's uncertainty.

-   It penalizes both the deviation of the prediction from the observed value and the sharpness of the predicted distribution.

Limitations:

-   CRPS is less intuitive than RMSE, making it harder to interpret for non-technical audiences.

-   It requires more complex calculations compared to RMSE.

```{r calculate CPRS}
CPRS_month_site <- reconstruct_months_summary$prediction_metrics %>%
  filter(month != "all", site != "all") %>%
  summarise(mean_pc_CRPS = mean(CRPS * 100))

CPRS_month <- reconstruct_months_summary$prediction_metrics %>%
  filter(site == "all") %>%
  summarise(mean_pc_CRPS_over_sites = mean(CRPS * 100))

### CPRS if assessing all site-month datapoints with no survey data
CPRS_month_site

### CPRS if only interested in overall monthly trends
CPRS_month
```

## **Incremental Value of ANC Data**

However, a clear question arises 'what is the ANC data actually adding to what we currently have??'. For this we need to define a 'null model' for what we would normally predict in the absence of the data. The choice of this is somewhat subjective but in our analysis we compare predictions made using ANC data alone with those where we extrapolate from the most recent available survey data.

### **Null Model (Survey Data Only)**

Here we continue with our example of a survey of three months duration every three years. In contrast, in our final analysis, we provided ANC data in Rarieda a much higher benchmark of being superior to a prediction based upon extrapolating community prevalence from the continuous Malaria Indicator Survey over a five month time horizon.

```{r fitting the null model for monthly prediction}
## create a dataset with ANC data removed
months_to_reconstruct_no_ANC <- months_to_reconstruct
months_to_reconstruct_no_ANC$data_to_model <- months_to_reconstruct_no_ANC$data_to_model %>% filter(ANC == 0)

### where there is no survey data model extrapolates from most recent available data
reconstruct_month_no_ANC <- predict_nearest_month(months_to_reconstruct_no_ANC$data_to_model, 500, 1000, 5)
## plot 
predict_month_no_ANC_plot$overall_plot

```

In this toy example monthly prevalence are uniformly distributed and completely uncorrelated so it is unsurprising the null model fairs poorly, in reality we'd expect malaria prevalence to exhibit much more correlation over time, meaning it should be more difficult for ANC data to provide added value - the more slowly malaria moves the harder it is for ANC data to provided added value.

### **CRPSS Calculation**

CRPSS (Continuous Ranked Probability Skill Score) provides a measure of the skill of a probabilistic forecast relative to a reference forecast (in this case, the null model). A CRPSS value of 100% would indicate a perfect forecast, while a value of 0% would indicate no improvement over the reference forecast. Unsurprisingly, in our toy example, the lack of predictive power of a previous community prevalence survey from one month to the next here means the CRPSS is very high. However, in our analysis we still find substantial improvements using ANC data to reconstruct community prevalence over a five month time horizon.

```{r CRPSS Calculation}

CRPSS <- (1 - CPRS_month$mean_pc_CRPS_over_sites / CPRS_month_null$mean_pc_CRPS_over_sites) * 100

CRPSS
```

## Defining a Null model for site-specific prevalence

For location-specific data, we use a null model whereby the prevalence in a given location without ANC is modeled by extrapolating from a simple random-effects model for the other locations in the region.

```{r fit site-specific null model}
## remove ANC data from dataset
sites_to_reconstruct_no_ANC <- sites_to_reconstruct
sites_to_reconstruct_no_ANC$data_to_model <- sites_to_reconstruct$data_to_model %>% filter(ANC == 0)
##
reconstruct_sites_no_ANC <- predict_RE_site(sites_to_reconstruct_no_ANC$data_to_model, 500, 1000, 5)
reconstruct_sites_no_ANC_summary <- get_fit_plots(reconstruct_sites_no_ANC, sites_to_reconstruct_no_ANC$simulated_survey_data, sites_to_reconstruct_no_ANC$param_df, pred_sites = sites_to_reconstruct_no_ANC$pred_sites)
reconstruct_sites_no_ANC_summary$overall_plot
```
