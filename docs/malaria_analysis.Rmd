---
title: "Assessing Malaria Prevalence: Community vs Antenatal Care in Rarieda Sub-county, Western Kenya"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
source("../R/setup.R")
source("../R/data_generation.R")
source("../R/models.R")
source("../R/summary_functions.R")
set.seed(101)
```

## **Introduction**

This analysis summarizes our approach to assessing the relationship between the prevalence of malaria within antenatal care (ANC) and the community in Rarieda sub-county, Western Kenya. Using toy examples, we will demonstrate how we fit and compare models of differing complexity, generate out-of-sample predictions based on ANC prevalence alone, and quantify the incremental value of ANC data compared to traditional population-based surveys of malaria prevalence in the community.

## **Generating Synthetic Data**

We begin by generating synthetic data representing the simplest and most complex models.These will serve as our toy-examples and allow us to verify that our code does what it should.

### **Model 1 - Simplest Model**

In this model, prevalence varies by site and month with a constant log-odds difference between ANC and community prevalence.

```{r Generate data from Model 1}
# Generating data for Model 1
m1_data <- generate_data_m1()
# Plotting the data
plot_m1_data<-get_data_plots(m1_data$data_to_model)
plot_m1_data$overall_plot
```

### **Model 5 - Most Complex Model**

In this model, prevalence varies by site and month with a linear relationship on the logit-scale, having different slopes and intercepts depending upon gravidity category.

```{r Generate data from Model 5}
# Generating data for Model 5
m5_data <- generate_data_m5()
# Plotting the data
plot_m5_data<-get_data_plots(m5_data$data_to_model)
plot_m5_data$overall_plot
```

## **Fitting Models**

Next, we fit the models to the generated data.

### **Fit Model 1**

```{r Fit Model 1}
m1_fitting <- fit_m1(m1_data$data_to_model, 1000, 1000, 10)
```

### **Fit Model 5**

```{r Fit Model 5}
m5_fitting <- fit_m5(m5_data$data_to_model, 1000, 1000, 10)
```

## **Model Evaluation**

We evaluate the models by comparing the fitted parameters to the simulated values.

### **Model 1 Evaluation**

```{r summarise Model 1 fit}
m1_summary <- get_fit_summary(m1_fitting, m1_data$simulated_survey_data, m1_data$param_df)
m1_summary$overall_plot
```

### **Model 5 Evaluation**

```{r summarise Model 5 fit}
m5_summary <- get_fit_summary(m5_fitting, m5_data$simulated_survey_data, m5_data$param_df)
m5_summary$overall_plot
```

## **Fitting Simple Model to Complex Data**

We fit the simplest model to data generated by the most complex model to highlight fitting metrics.

```{r fit simpler model to data generated with more complex model}
m1_fit_to_m5_data <- fit_m1(m5_data$data_to_model, 1000, 1000, 10)
m1_fit_m5_data_summary <- get_fit_summary(m1_fit_to_m5_data, m5_data$simulated_survey_data, m5_data$param_df)
m1_fit_m5_data_summary$overall_plot
```

### **Model Comparison**

We compare the effective number of parameters and the Deviance Information Criterion (DIC) between models. Here the difference between complexity of the model could be expected to be approximately (though not exactly) six, reflecting the difference in number of specified parameters. The DIC metric balances the improvement of fit versus the potential for overfitting. Here, fitting Model 1 to data generated by Model 5 produces a substantially worse fit than fitting the correct model, as expected

```{r compare fitting metrics}
## difference in effective parameters 
m5_fitting$p_d - m1_fit_to_m5_data$p_d

## a better fitting model should have a lower DIC 
m5_fitting$DIC - m1_fit_to_m5_data$DIC
```

## **Out-of-Sample Predictions**

We now assess the extent to which models can predict out-of-sample. Here, we reconstruct ANC prevalence between surveys spaced over a few months every few years.

When fitting our models, we use random effects for site and month to capture the inherent variability and unobserved heterogeneity across different sites and months. Random effects allow us to generalize our findings beyond the specific sites and months in our dataset, providing a more robust model that accounts for potential variability in new data.

However, for reconstructing community prevalence in months or sites in which data are not available, we want to relax the assumption of normality in these effects. To demonstrate this, we simulate log-odds ratios (log ORs) by month (if predicting month) or by site (if predicting site) that are non-normally distributed, specifically using a uniform distribution from 0 to 1. The model for prediction then uses fixed effects to provide the maximum flexibility in prediction based on the observed data. This approach allows us to capture the full range of possible values without being constrained by the normality assumption.

### **Generate Data for reconstructing data by month**

```{r Generate data for monthly predictions}
## will generate a dataset with a default of only including community prevalence for 3 months every 3 years
months_to_reconstruct <- generate_data_m1_runif_month()
```

### Reconstructing

```{r reconstruct monthly}
### 
reconstruct_months <- predict_months_m1(months_to_reconstruct$data_to_model, 1000, 1000, 10)
reconstruct_months_summary <- get_fit_summary(reconstruct_months, months_to_reconstruct$simulated_survey_data, months_to_reconstruct$param_df, pred_months = months_to_reconstruct$pred_months)
reconstruct_months_summary$overall_plot
```

### Similarly reconstructing site-level community prevalence using only ANC data

```{r reconstruct site}
## default just arbitralily removes survey data in sites 15-20 from fitting
sites_to_reconstruct <- generate_data_m1_runif_site()

reconstruct_sites <- predict_sites_m1(sites_to_reconstruct$data_to_model, 1000, 1000, 10)
reconstruct_sites_summary <- get_fit_summary(reconstruct_sites, sites_to_reconstruct$simulated_survey_data, sites_to_reconstruct$param_df, pred_sites = sites_to_reconstruct$pred_sites)
reconstruct_sites_summary$overall_plot
```

## **Quantitative Assessment of our reconstructions of community prevalence**

We present two metrics to assess the predictive accuracy: Residual Mean Square Error (RMSE) and Continuous Ranked Probability Score (CRPS).

### **RMSE**

RMSE measures the absolute difference between predicted and observed prevalence. It penalizes predictions that are further away more heavily. This makes RMSE particularly sensitive to outliers and large errors, which can be both a strength and a limitation.

Strengths:

-   RMSE is straightforward to interpret: lower RMSE values indicate better model performance.

-   As a distance measure involving % prevalence it is likely more intuitive and involves a metric typically used to stratify endemicity levels.

-   Heavily penalizes large errors, which can be useful in applications where such errors are particularly costly.

Limitations:

-   RMSE is sensitive to outliers, which can disproportionately affect the metric.

-   RMSE typically isn't recommended for assessing the validity of a probability metric. For example, in a situation where a prediction ranges between 47% and 53% for a 50% prevalence, this may be reasonable. However, for a setting with 1% prevalence, predictions ranging from -2% to 4% are substantially less useful, yet all four of these predictions have the same RMSE.

-   It is not a proper scoring rule for probabilistic forecasts because it does not account for the predicted distribution's uncertainty.

```{r summarise monthly reconstruction }
#RMSE for every site-month
reconstruct_months_summary$prediction_metrics %>%   filter(month != "all", site != "all") %>%   summarise(mean_pc_RMSE = mean(RMSE * 100)) 
#RMSE for overall monthly prevalence 
reconstruct_months_summary$prediction_metrics %>%   filter(site == "all") %>%   summarise(mean_pc_RMSE_over_sites = mean(RMSE * 100)) 
```

### **CRPS**

CRPS is a more technically valid metric for assessing the accuracy of probability predictions. It generalizes the concept of RMSE to probabilistic forecasts, providing a more nuanced view.

Strengths:

-   CRPS is a proper scoring rule, meaning it appropriately accounts for the predicted distribution's uncertainty.

-   It penalizes both the deviation of the prediction from the observed value and the sharpness of the predicted distribution.

Limitations:

-   CRPS is less intuitive than RMSE, making it harder to interpret for non-technical audiences.

-   It requires more complex calculations compared to RMSE.

```{r calculate CPRS}
CRPS_month_site <- reconstruct_months_summary$prediction_metrics %>%
  filter(month != "all", site != "all") %>%
  summarise(mean_pc_CRPS = mean(CRPS * 100))

CRPS_month <- reconstruct_months_summary$prediction_metrics %>%
  filter(site == "all") %>%
  summarise(mean_pc_CRPS_over_sites = mean(CRPS * 100))

### CPRS if assessing all site-month datapoints with no survey data
CRPS_month_site

### CPRS if only interested in overall monthly trends
CRPS_month
```

## **Incremental Value of ANC Data**

However, a clear question arises 'what is the ANC data actually adding to what we currently have??'. For this we need to define a 'null model' for what we would normally predict in the absence of the data. The choice of this is somewhat subjective but in our analysis we compare predictions made using ANC data alone with those where we extrapolate from the most recent available survey data.

### **Null Model (Survey Data Only)**

Here we continue with our example of a survey of three months duration every three years. In contrast, in our final analysis, we provided ANC data in Rarieda a much higher benchmark of being superior to a prediction based upon extrapolating community prevalence from the continuous Malaria Indicator Survey over a five month time horizon.

```{r fitting the null model for monthly prediction}
## create a dataset with ANC data removed
months_to_reconstruct_no_ANC <- months_to_reconstruct
months_to_reconstruct_no_ANC$data_to_model <- months_to_reconstruct_no_ANC$data_to_model %>% filter(ANC == 0)

### where there is no survey data model extrapolates from most recent available data
reconstruct_month_no_ANC <- predict_nearest_month(months_to_reconstruct_no_ANC$data_to_model, 1000, 1000, 10)
## plot 
reconstruct_month_no_ANC_summary<-get_fit_summary(reconstruct_month_no_ANC,months_to_reconstruct_no_ANC$simulated_survey_data,months_to_reconstruct_no_ANC$param_df,pred_months=months_to_reconstruct_no_ANC$pred_months)
reconstruct_month_no_ANC_summary$overall_plot

```

In this toy example monthly prevalence are uniformly distributed and completely uncorrelated so it is unsurprising the null model fairs poorly, in reality we'd expect malaria prevalence to exhibit much more correlation over time, meaning it should be more difficult for ANC data to provide added value - the more slowly malaria moves the harder it is for ANC data to provided added value.

### **CRPSS Calculation**

CRPSS (Continuous Ranked Probability Skill Score) provides a measure of the skill of a probabilistic forecast relative to a reference forecast (in this case, the null model). A CRPSS value of 100% would indicate a perfect forecast, while a value of 0% would indicate no improvement over the reference forecast. Unsurprisingly, in our toy example, where the model generating the data uses an independent draw from a uniform distribution between 0 and 1 for each month, the lack of predictive power of a previous community prevalence survey from one month to the next here means the CRPSS is very high. However, in our analysis we still find substantial improvements using ANC data to reconstruct community prevalence over a five month time horizon.

```{r CRPSS Calculation}
CRPS_month_null<-reconstruct_month_no_ANC_summary$prediction_metrics%>%
  filter(site=="all")%>%
  summarise(mean_pc_CRPS_over_sites=mean(CRPS*100))

CRPSS <- (1 - CRPS_month$mean_pc_CRPS_over_sites / CRPS_month_null$mean_pc_CRPS_over_sites) * 100

CRPSS
```

## Defining a Null model for site-specific prevalence

For location-specific data, we use a null model whereby the prevalence in a given location without ANC is modeled by extrapolating from a simple random-effects model for the other locations in the region.

```{r fit site-specific null model}
## remove ANC data from dataset
sites_to_reconstruct_no_ANC <- sites_to_reconstruct
sites_to_reconstruct_no_ANC$data_to_model <- sites_to_reconstruct$data_to_model %>% filter(ANC == 0)
## reconstruct community prevalence from mean and SD of other sub-locations in the area
reconstruct_sites_no_ANC <- predict_RE_site(sites_to_reconstruct_no_ANC$data_to_model, 1000, 1000, 10)
reconstruct_sites_no_ANC_summary <- get_fit_summary(reconstruct_sites_no_ANC, sites_to_reconstruct_no_ANC$simulated_survey_data, sites_to_reconstruct_no_ANC$param_df, pred_sites = sites_to_reconstruct_no_ANC$pred_sites)
## plot
reconstruct_sites_no_ANC_summary$site_plot
```

Again, we are giving our toy example a much easier task to beat our null model as the model generating the data assumes prevalence in a specific site is completely random between 0 and 1. In reality prevalence is likely to be far more heavily correlated between a particular sub-location and those which surround it.

However, in our analysis this would provide much more context-specific information compared to relying upon a typical MIS - our null model involves extrapolate from data from x thousand samples from a population of y sampled from all villages outside of the sub-location. In contrast the most recent Kenya MIS has data from x children to represent a population of y million in the Lake region of the country.

We can also explore both the absolute and incremental fit as with our monthly reconstructions

```{r CRPS site}
CRPS_site <- reconstruct_sites_summary$prediction_metrics %>%
  filter(site == "all") %>%
  summarise(mean_pc_CRPS_over_months = mean(CRPS * 100))
CRPS_site_null <- reconstruct_sites_no_ANC_summary$prediction_metrics %>%
  filter(site == "all") %>%
  summarise(mean_pc_CRPS_over_months = mean(CRPS * 100))

CRPSS_site <- (1 - CRPS_site$mean_pc_CRPS_over_months / CRPS_site_null$mean_pc_CRPS_over_months) * 100
CRPSS_site

```
